{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Do! 프로젝트 root 경로로 설정\n",
    "project_path = \"C:/workspace/Bus Project\"\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import bus.analyzer as anz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) grouping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_column(df):\n",
    "    df['cluster_group'] = df['station_address'].apply(lambda x : x.split(\" \")[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) clustering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_s = 'cluster_target'\n",
    "level_s = 'cluster_level'\n",
    "earth_radius = 6371.0088 # 단위: km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_level_spatial_dbscan_result \\\n",
    "    (df, n, eps, min_pts = 3, nonf_cols = ['station_id', 'station_name'], f_cols = ['station_longitude', 'station_latitude']):\n",
    "\n",
    "    # clustering\n",
    "    global earth_radius \n",
    "    dbscan = DBSCAN(eps = eps/1000/earth_radius, algorithm='ball_tree', \n",
    "                    metric='haversine', min_samples=min_pts)\n",
    "    \n",
    "    temp_df = df.loc[:,nonf_cols + f_cols]\n",
    "    temp_df.loc[:, target_s] = dbscan.fit_predict(np.radians(temp_df[f_cols]))\n",
    "\n",
    "    # level 부여\n",
    "    success_index = temp_df.query(target_s + ' > -1').index\n",
    "    temp_df.loc[success_index, level_s] = str(n)\n",
    "    \n",
    "    failed_index = set(temp_df.index) - set(success_index)\n",
    "    temp_df.loc[failed_index, level_s] = str(-1)\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_handled_result \\\n",
    "    (df, n, by1='station_id', by2='station_name', f_cols=['station_longitude', 'station_latitude']):\n",
    "    temp_df = df.loc[:, [by1, by2] + f_cols]\n",
    "\n",
    "    temp_df['cluster_target'] = -1\n",
    "    temp_df['cluster_level'] = -1\n",
    "\n",
    "    grouped_df = temp_df.groupby(by = by2, as_index = False)\n",
    "    grouped_df_count = grouped_df.count()\n",
    "    station_nm_list = list(grouped_df_count[grouped_df_count[by1] >= 2][by2])\n",
    "    target_range = np.arange(len(station_nm_list))\n",
    "\n",
    "    for i in target_range:\n",
    "        list_idx = temp_df.query('%s == \"%s\"' % (by2, station_nm_list[i])).index\n",
    "        for idx in list_idx:\n",
    "            temp_df.loc[idx, 'cluster_target'] = i\n",
    "            temp_df.loc[idx, 'cluster_level'] = n\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_result(df, n):\n",
    "    df.loc[:, 'cluster_level'] = n\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_dbscan_result(eps, station_df):\n",
    "    df_lv1_group = get_n_level_spatial_dbscan_result(station_df, 1, eps, min_pts = 3)\n",
    "    \n",
    "    df_lv2_group = df_lv1_group.query(target_s + ' == -1')\n",
    "    df_lv2_group = get_n_level_spatial_dbscan_result(df_lv2_group, 2, eps, min_pts = 2)\n",
    "    \n",
    "    df_noise_handled_group = df_lv2_group.query(target_s + ' == -1')\n",
    "    df_noise_handled_group = get_noise_handled_result(df_noise_handled_group, 3)\n",
    "    \n",
    "    df_noise_group = df_noise_handled_group.query(target_s + ' == -1')\n",
    "    df_noise_group = get_noise_result(df_noise_group, 4)\n",
    "    \n",
    "    r1 = df_lv1_group.query(target_s + ' > -1')\n",
    "    r2 = df_lv2_group.query(target_s + ' > -1')\n",
    "    r3 = df_noise_handled_group.query(target_s + ' > -1')\n",
    "    r4 = df_noise_group\n",
    "    \n",
    "    combined = pd.concat([r1, r2, r3, r4])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hooni\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\hooni\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "station_df = anz.load_station_df()\n",
    "\n",
    "station_df = station_df[station_df['station_id'] != 0] ####################\n",
    "\n",
    "station_df = make_group_column(station_df)\n",
    "\n",
    "group_list = list(station_df[\"cluster_group\"].drop_duplicates())\n",
    "group_df_list = []\n",
    "for label in group_list:\n",
    "    group_df = station_df[station_df[\"cluster_group\"] == label]\n",
    "    group_df_list.append(group_df)\n",
    "    \n",
    "eps_list = [105, 90]\n",
    "# eps_list = [300, 200]\n",
    "cluster_df_list = []\n",
    "\n",
    "for i, group_df in enumerate(group_df_list):   \n",
    "    eps = eps_list[i]\n",
    "    # eps에 따른 dbscan 수행. => label OR noise 생성\n",
    "    cluster_df = get_spatial_dbscan_result(eps, group_df)\n",
    "\n",
    "    cluster_df['cluster_level'] = cluster_df['cluster_level'].astype(str)\n",
    "    cluster_df['cluster_target'] = cluster_df['cluster_target'].astype(str)\n",
    "    cluster_df['level_target'] = cluster_df['cluster_level'] + '&' + cluster_df['cluster_target']\n",
    "    \n",
    "    # noise 군집에 개별 레이블 부여\n",
    "    cluster_df[\"cluster_target\"] = cluster_df[\"cluster_target\"].apply(lambda x : int(x))\n",
    "    next_target = int(max(list(cluster_df[\"cluster_target\"]))) + 1\n",
    "    for j in cluster_df.index:\n",
    "        if cluster_df.loc[j, \"cluster_target\"] == -1:\n",
    "            cluster_df.loc[j, \"cluster_target\"] = next_target\n",
    "            next_target += 1\n",
    "    \n",
    "    # 군집 위치 구하기\n",
    "    cluster_df[\"cluster_longitude\"] = 0\n",
    "    cluster_df[\"cluster_latitude\"] = 0\n",
    "    target_list = set(cluster_df[\"cluster_target\"])\n",
    "    for target in target_list:\n",
    "        target_df = cluster_df[cluster_df[\"cluster_target\"] == target]\n",
    "        cluster_longitude = target_df[\"station_longitude\"].mean()\n",
    "        cluster_latitude = target_df[\"station_latitude\"].mean()\n",
    "        cluster_df.loc[cluster_df[\"cluster_target\"] == target, \"cluster_longitude\"] = cluster_longitude\n",
    "        cluster_df.loc[cluster_df[\"cluster_target\"] == target, \"cluster_latitude\"] = cluster_latitude\n",
    "\n",
    "    cluster_df[\"cluster_group\"] = group_list[i]    \n",
    "    cluster_df_list.append(cluster_df)\n",
    "\n",
    "base_id = 0\n",
    "for i, cluster_df in enumerate(cluster_df_list):\n",
    "    cluster_df[\"cluster_id\"] = cluster_df[\"cluster_target\"]+base_id\n",
    "    length = len(cluster_df[\"cluster_target\"].drop_duplicates())\n",
    "    base_id += length\n",
    "    \n",
    "# 그룹별 데이터를 하나로 묶음\n",
    "cluster_df = pd.concat(cluster_df_list)\n",
    "cluster_df = cluster_df[[\"station_id\", \"cluster_id\", \"cluster_group\",\"cluster_target\", \"cluster_longitude\", \"cluster_latitude\"]]\n",
    "cluster_station_df = cluster_df[[\"station_id\", \"cluster_id\"]]\n",
    "cluster_station_df.to_csv(\"data/analysis/cluster_station_df.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = anz.load_station_df()\n",
    "merged_df = pd.merge(station_df, cluster_df, on='station_id')\n",
    "root = merged_df[['cluster_id', 'cluster_group', 'cluster_target', 'cluster_longitude', 'cluster_latitude']].drop_duplicates().sort_values(['cluster_group', 'cluster_target'])\n",
    "branch = merged_df.groupby(by=[\"cluster_group\", \"cluster_target\"]).sum().reset_index()[['cluster_group', 'cluster_target', 'tour_geton_usage', 'regident_geton_usage', 'tour_getoff_usage', 'regident_getoff_usage', 'total_usage']]\n",
    "cluster_df = pd.merge(root, branch, on=['cluster_group', 'cluster_target'])\n",
    "cluster_df_selector = {}\n",
    "cluster_df = cluster_df.sort_values(by='cluster_id')\n",
    "cluster_df.to_csv(\"data/analysis/cluster_df.csv\", encoding=\"CP949\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
